# Hate Speech Classifier
# Description: A Bayesian classifier to determine whether or not a tweet
# contains hate speech, making use of the annotations from the UghML project
# Author: Annie Thorburn

import os, nltk, random
import xml.etree.ElementTree as ET
from os.path import join as pjoin

# Function to return the indices of all tweets containing a particular tag,
# given the tag's span(s) and a list of tweet boundaries
# Code for this function is borrowed from 'fleiss.py', written by Jake Freyer
# Used with permission of the original author
def find_tweets (spans, boundaries):
    spans = [[int (x) for x in span .split ('~')] for span in spans .split (',')]
    return sum ([[i for i in range (len (boundaries) - 1) \
                  if boundaries [i] <= span [1] and boundaries [i + 1] > span [0]] for span in spans], [])

# Initialize a dictionary that will have the set of tweets as its keys
# and a list of the tags for each tweet as the value for that tweet
taggedTweets = {}

# Iterate over the files in the GoldStandards folder
for filename in os.listdir('GoldStandards'):
    abs_name = os.path.abspath(pjoin('GoldStandards', filename))
    tree = ET.parse(abs_name)

    root = tree.getroot()
    # XML files generated by MAE are structured such that, when
    # parsed as a tree, the root has two children. The first
    # child contains the text, and the second contains the tags.
    text = root[0].text

    # Separate the text into a list of tweets, and find the indices of their
    # boundaries
    tweets = text.split('\n')
    tweets = [x for x in tweets if x != '']

    # In the tweet file from the first week, the tweets past the 51st tweet were
    # not annotated and should not be included in the data.
    # (Later weeks also contain some tweets with no tags, but these are usually
    # untagged because they did not contain anything the annotators thought should
    # be tagged, rather than because the annotators didn't look at them, so they
    # can still be used by the classifier and should be kept in the data.)
    if filename == 'Week1_Gold.xml':
        tweets = tweets[:51]

    # Add each tweet to the dictionary, with an empty list for its value
    # The tags will be added to the values later
    for tweet in tweets:
        taggedTweets[tweet] = []
        
    # Generate a list of the indices of the tweet boundaries
    # This will be used to determine which tweet each tag belongs to
    boundaries = [text.find(tweet) for tweet in tweets]
    boundaries.append(len(text))
    
    tags = [(t.tag, t.attrib) for t in root[1]]
    # For each tag, find the tweet that it corresponds to and add the tag
    # to that tweet's entry in the taggedTweets dictionary
    for tag in tags:
        # If the tag is a reference tag, find the tags it refers to
        # and use them to determine what tweet it refers to
        if tag[0] == 'Ref':
            if 'fromID' in tag[1].keys() and 'toID' in tag[1].keys():
                fromID = tag[1]['fromID']
                toID = tag[1]['toID']
                for tag2 in tags:
                    if tag2[0] != 'Ref':
                        if tag2[1]['id'] == fromID:
                            fromTag = tag2
                        if tag2[1]['id'] == toID:
                            toTag = tag2
                            
                i = find_tweets(fromTag[1]['spans'], boundaries)
                j = find_tweets(toTag[1]['spans'], boundaries)
                if i != []:
                    tweet = tweets[i[0]]
                elif j != []:
                    tweet = tweets[j[0]]
                # If either the 'from' tag or the 'to' tag is non-consuming,
                # add it to the tweet's entry now, since it will not be
                # added when we add the non-ref tags
                if i == []:
                    taggedTweets[tweet].append(fromTag)
                if j == []:
                    taggedTweets[tweet].append(toTag)

        # Add the non-reference tags (Groups, Sentiments, and Stereotypes)                        
        else:
            i = find_tweets(tag[1]['spans'], boundaries)
            if i != []:
                tweet = tweets[i[0]]
                taggedTweets[tweet].append(tag)

# Generate a dictionary of tweets and their categories (hate or nonhate)
# Use the data from the tags to categorize a tweet as hateful or nonhateful
categories = {}
for tweet in taggedTweets.keys():
    categories[tweet] = 'nonhate'
    for tag in taggedTweets[tweet]:
        if tag[0] == 'Group':
            if tag[1]['hate'] == 'yes':
                categories[tweet] = 'hate'
        elif tag[0] == 'Stereotype' or tag[0] == 'Sentiment':
            if 'nonhate' not in tag[1].keys():
                categories[tweet] = 'hate'

# Divide the data into a training set (8/10 of the data)
# and a test set (2/10 of the data)
# Randomly choose which tweets will be assigned to each set, to avoid overfitting
l = taggedTweets.keys()
random.shuffle(l)
i = len(l) * 8/10
trainSet = {}
testSet = {}
for t in l[:i]:
    trainSet[t] = taggedTweets[t]
for t in l[i:]:
    testSet[t] = taggedTweets[t]

# BASELINE CLASSIFIER
# This classifier uses only the raw word features of a tweet to classify it

# Create a set of all the words in the corpus
# This will be used to identify the word features for each tweet
featureWords = []
for tweet in taggedTweets.keys():
    # Split the text and clean it up by removing punctuation and
    # making each word lowercase
    words = [w.strip('"\\.,:/!?\'()').lower() for w in tweet]
    featureWords.extend(words)
featureWords = set(featureWords)

# Function to obtain the word features from a tweet
def wordFeatures(tweet):
    tweetWords = [w.strip('"\\.,:/!?\'()').lower() for w in tweet]
    features = {}
    for word in featureWords:
        features['contains(%s)' % word] = (word in tweetWords)
    return features

# Generate the training and test data: for each tweet, create a tuple
# containing that tweet's features and its category
baseTrainData = [(wordFeatures(tweet), categories[tweet]) for tweet in trainSet.keys()]
baseTestData = [(wordFeatures(tweet), categories[tweet]) for tweet in testSet.keys()]

# Train a classifier on the training data and test it on the test data
# Report the accuracy and the 10 most informative features
baseline = nltk.NaiveBayesClassifier.train(baseTrainData)
acc = nltk.classify.accuracy(baseline, baseTestData)
print ('The accuracy of the baseline classifier is %s' % acc)
baseline.show_most_informative_features(10)

# ADVANCED CLASSIFIER
# This classifier makes use of the annotations when classifying a tweet

# Extract features for the classifier from the dictionary of tagged tweets
groupWords = []
groupBigrams = []
groupTrigrams = []
stereotypeWords = []
stereotypeBigrams = []
stereotypeTrigrams = []
sentimentWords = []
sentimentBigrams = []
sentimentTrigrams = []

for tweet in trainSet.keys():
    for tag in taggedTweets[tweet]:
        if tag[0] == 'Group':
            # Split the text and clean it up by removing punctuation and
            # making each word lowercase
            text = [w.strip('"\\.,:/!?\'()').lower() for w in tag[1]['text'].split()]
            groupWords.extend(text)
            groupBigrams.extend(list(nltk.bigrams(text)))
            groupTrigrams.extend(list(nltk.trigrams(text)))
        elif tag[0] == 'Stereotype':
            text = [w.strip('"\\.,:/!?\'()').lower() for w in tag[1]['text'].split()]
            stereotypeWords.extend(text)
            stereotypeBigrams.extend(list(nltk.bigrams(text)))
            stereotypeTrigrams.extend(list(nltk.trigrams(text)))
        elif tag[0] == 'Sentiment':
            text = [w.strip('"\\.,:/!?\'()').lower() for w in tag[1]['text'].split()]
            sentimentWords.extend(text)
            sentimentBigrams.extend(list(nltk.bigrams(text)))
            sentimentTrigrams.extend(list(nltk.trigrams(text)))
            
groupWords = set(groupWords)
groupBigrams = set(groupBigrams)
groupTrigrams = set(groupTrigrams)
stereotypeWords = set(stereotypeWords)
stereotypeBigrams = set(stereotypeBigrams)
stereotypeTrigrams = set(stereotypeTrigrams)
sentimentWords = set(sentimentWords)
sentimentBigrams = set(sentimentBigrams)
sentimentTrigrams = set(sentimentTrigrams)

# Function to extract features from a tweet and its tags
def tweetFeatures(tweet):
    features = {}
    tweetWords = [w.strip('"\\.,:/!?\'()').lower() for w in tweet]
    tweetBigrams = list(nltk.bigrams(tweetWords))
    tweetTrigrams = list(nltk.trigrams(tweetWords))
    for word in groupWords:
        features['contains(%s)' % word] = (word in tweetWords)
    for word in stereotypeWords:
        features['contains(%s)' % word] = (word in tweetWords)
    for word in sentimentWords:
        features['contains(%s)' % word] = (word in tweetWords)
    for bigram in groupBigrams:
        features['contains(%s)' % str(bigram)] = (bigram in tweetBigrams)
    for bigram in stereotypeBigrams:
        features['contains(%s)' % str(bigram)] = (bigram in tweetBigrams)
    for bigram in sentimentBigrams:
        features['contains(%s)' % str(bigram)] = (bigram in tweetBigrams)
    for trigram in groupTrigrams:
        features['contains(%s)' % str(trigram)] = (trigram in tweetTrigrams)
    for trigram in stereotypeTrigrams:
        features['contains(%s)' % str(trigram)] = (trigram in tweetTrigrams)
    for trigram in sentimentTrigrams:
        features['contains(%s)' % str(trigram)] = (trigram in tweetTrigrams)
    return features

# Generate the training and test data: for each tweet, create a tuple
# containing that tweet's features and its category
trainData = [(tweetFeatures(tweet), categories[tweet]) for tweet in trainSet.keys()]
testData = [(tweetFeatures(tweet), categories[tweet]) for tweet in testSet.keys()]

# Train a classifier on the training data and test it on the test data
# Report the accuracy and the 10 most informative features
classifier = nltk.NaiveBayesClassifier.train(testData)
acc = nltk.classify.accuracy(classifier, testData)
print ( 'The accuracy of the classifier is %s' % acc)
classifier.show_most_informative_features(10)
